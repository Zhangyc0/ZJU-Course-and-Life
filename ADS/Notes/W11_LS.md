[TOC]

é€šè¿‡è¿­ä»£çš„æ–¹å¼æé«˜æ­£ç¡®ç‡ã€‚ä¾‹å¦‚æœ¬æ¥æ­£ç¡®ç‡æ˜¯2/3ï¼Œåˆ™è¿­ä»£kæ¬¡åå°±æœ‰(1-2/3)^k^

---

## Intro

Concentrate on Local optimum, step by step until there is no improvement

### Framwork

* Local
    * Define <u>neighborhoods</u> in the feasible set
        * N(S): neighborhood of S
    * A local optimum is a best solution in a neighborhood
* Search
    * Start with a feasible solution and search a better one within the neighborhood
    * A local optimum is achieved if no improvement is possible

### Neighbor Relation

S ~ S': S' is a neighboring solution of S, S' can be obtained by a small modification of S.

N(S): neighborhood of S â€“ the set { S': S ~ S' }.

$\mathcal{FS}$: Feasible Solution set

é‚£ä¸ªç¢—çš„ä¾‹å­ä¸­ç”¨çš„æ¢¯åº¦ä¸‹é™æ³•æ²¡é—®é¢˜ï¼Œ<u>å› ä¸ºå®ƒä¸å­˜åœ¨å±€éƒ¨æœ€ä¼˜è§£</u>

```pseudocode
SolutionType Gradient_descent()
{   Start from a feasible solution S âˆˆ FS ;
    MinCost = cost(S);
    while (1) {
        Sâ€™ = Search( N(S) ); /* find the best Sâ€™ in N(S) */
        CurrentCost = cost(Sâ€™);
        if ( CurrentCost < MinCost ) {
            MinCost = CurrentCost;
            S = Sâ€™;
        }
        else
            break;
    }
    return S;
}
```



## Vertex Cover

> ~~>Decisionï¼šæ˜¯å¦å­˜åœ¨è‡³å¤škä¸ªé¡¶ç‚¹çš„å­å›¾ï¼Œä½¿å¾—åŸå›¾ä¸­æ¯æ¡è¾¹è‡³å°‘æœ‰ä¸€ä¸ªé¡¶ç‚¹åœ¨å­å›¾ä¸­(å³è¦†ç›–äº†æ‰€æœ‰çš„è¾¹)~~
>
> Optimization: Given an undirected graph G = (V, E).  Find a minimum subset S of  V such that for each edge (u, v) in E, either u or v  is in S.
>

Feasible solution set $\mathcal{FS}$: all the vertex covers

cost(S) = |S|

S~S': <u>S' can be obtained from S by (adding or) deleting a **single** node.</u>  (å¦‚æœN(S)æ˜¯é‚»ç‚¹é›†çš„è¯é‚£å°±åªèƒ½æ˜¯åˆ ç›¸é‚»ç‚¹äº†)
\\         Each vertex cover S has at most |V| neighbors. (each vertex can be deleted once)

Search: Start from S = V; delete a node and check if S' is a vertex cover <u>with a smaller cost</u>.



**Ptifalls of Gradient-Descent algorithm**

<img src="assets/image-20200515152703687.png" style="zoom: 25%;" />

### Metropolis Algorithm

Metropolisæ˜¯äººå

é¿å…è½å…¥å±€éƒ¨æœ€ä¼˜çš„æƒ…å†µ

```pseudocode
/* åªæœ‰åŠ æ³¨é‡Šçš„è¿™ä¸¤è¡Œæ˜¯å’ŒGradient_descentç®—æ³•ä¸åŒçš„ */
SolutionType Metropolis()
{   Define constants k and T;
    Start from a feasible solution S âˆˆ FS;
    MinCost = cost(S);
    while (1) {
        Sâ€™ = Randomly chosen from N(S);	/* éšæœºé€‰ä¸€ä¸ª */
        CurrentCost = cost(Sâ€™);
        if ( CurrentCost < MinCost ) {
            MinCost = CurrentCost;
            S = Sâ€™;
        } else {
            With a probability e^{-Î”cost/(kT)}, let S = Sâ€™;	/* Randomï¼Œé¿å…é”æ­» */
            else break;
        }
    }
    return S;
}
```

æ³¨ï¼šå¯¹äºä¸Šé¢çš„case 1ï¼Œæœ‰ä¸€å®šæ¦‚ç‡å¯ä»¥è·³å‡ºlocal optimumå¾—åˆ°æ­£ç¡®è§£ã€‚ä½†æ˜¯å¯¹case 0ï¼Œæœ‰å¯èƒ½åœ¨åŠ 1å’Œå‡1ä¹‹é—´æ— é™éœ‡è¡â€¦â€¦

æ³¨ï¼šå½“ï¼ˆæ¸©åº¦ï¼‰Tå¾ˆé«˜æ—¶ï¼Œä¸Šå¡çš„æ¦‚ç‡($e^{-\Delta cost/(kT)}$)å‡ ä¹ä¸º1ï¼Œå®¹æ˜“å¼•èµ·åº•éƒ¨éœ‡è¡ï¼›å½“Tæ¥è¿‘0æ—¶ï¼Œä¸Šå¡æ¦‚ç‡å‡ ä¹ä¸º0ï¼Œ<u>æ¥è¿‘åŸå§‹çš„æ¢¯åº¦ä¸‹é™æ³•</u>ã€‚

### Simulated Annealing

æ¸©åº¦æ…¢æ…¢é™ä½ï¼Œè·å¾—æœ€ä¼˜è§£

## Hopfield Neural Networks

> Graph G = (V, E) with integer edge weights w (positive or negative).
>
> If we < 0, where e = (u, v), then u and v want to have the same state;if we > 0 then u and v want different states.
>
> The absolute value |we| indicates the strength of this requirement.
>
> Output: A configuration S of the network ï¼šâ€œâ€“ an assignment of the state su to each node u (There may be no configuration that respects the requirements imposed by all the edges.)
> 
> So just <u>Find a configuration that is sufficiently good.</u>
>
> ---
>
> edge e(u,v) is <u>good</u>: $\large w_es_us_v<0$
>
> a node u is <u>satisified</u>: $\large \displaystyle \sum_{v:e(u,v) \in E} w_es_us_v \leqslant 0$
>
> configuration is <u>stable</u>: all nodes are satisfied.
>

Does a Hopfield network always have a stable configuration, and if so, how can we find one?

### State-flipping Algorithm

æ‰¾åˆ°unsatisfiedçš„æ¢è‰²

```pseudocode
ConfigType State_flipping()
{
    Start from an arbitrary configuration S;
    while ( ! IsStable(S) ) {
        u = GetUnsatisfied(S);
        su = - su;	/* flip unsatisfied point's color */
    }
    return S;
}
```

ä¸ä¼šæ­»å¾ªç¯ï¼Œå› ä¸ºå¯¹äºç›®æ ‡å‡½æ•°$\large \Phi(S) = \sum_{e\ {\rm is\ good}}|w_e|$ï¼Œå½“å…¶ä»Så˜æˆS'ä¹‹åï¼Œæœ‰ä¸€ä¸ªç‚¹çš„é¢œè‰²ç¿»è½¬äº†ï¼Œä¸ä¹‹ç›¸è¿çš„æ‰€æœ‰goodè¾¹å’Œbadè¾¹çš„å±æ€§éƒ½åè½¬ï¼Œä½†æ˜¯å…¶ä»–è¾¹å±æ€§ä¸å˜ï¼Œå› æ­¤æœ‰ï¼š$\displaystyle \Phi(S') = \Phi(S) - \sum_{e \in E\ is\ bad}|w_e| + \sum_{e \in E\ is\ good}|w_e|(Eä¸ºS'ä¸­uçš„é‚»è¾¹é›†)$ï¼Œåˆå› ä¸ºæœ¬æ¥uæ˜¯unsatisfiedçš„ï¼Œå› æ­¤æœ¬æ¥çš„badè¾¹(ç°åœ¨çš„good)çš„æƒé‡ä¹‹å’Œè‚¯å®šè¶…è¿‡goodè¾¹(ç°åœ¨çš„bad)çš„ï¼Œå› æ­¤åé¢é‚£é¡¹æ˜¯æ­£çš„ï¼Œå› æ­¤$\Phi(S)$æ˜¯é€’å¢çš„

**Local Search**

Problem: maximize Ï†

FS: configurations

S~S': S' can be obtained from S by flipping a single state

## The Max Cut Problem

> <u>æ˜¯Hopfieldçš„ç‰¹æ®Šæƒ…å†µ (æ‰€æœ‰è¾¹éƒ½æ˜¯æ­£è¾¹æƒ)</u>
>
> Maximum Cut problem: Given an undirected graph G = (V, E) with positive integer edge weights w~e~, find a node partition (A, B) such that the total weight of edges crossing the cut is maximized.
>
> $\displaystyle w(A,B) = \sum_{u \in A, v \in B} w_{(u, v)}$
>
> * Toy application
>     * n activities, m people.
>     * Each person wants to participate in two of the activities.
>     * Schedule each activity in the morning or afternoon to maximize number of people that can enjoy both activities.
> * Real applications: Circuit layout, statistical physics

**Local Search**

Problem:  To maximize w(A, B).

FS: any partition (A, B) 

S~S': S'(Single-flip neighborhood) can be obtained from S by moving one node from A to B, or one from B to A.

è¿™ä¸ªç®—æ³•å’Œä¸Šé¢é‚£ä¸ªç¥ç»ç½‘ç»œçš„ä¸€æ¨¡ä¸€æ ·

```pseudocode
ConfigType State_flipping()
{
    Start from an arbitrary configuration S;
    while ( ! IsStable(S) ) {
        u = GetUnsatisfied(S);
        su = -su;
    }
    return S;
}
```

### Approx-Ratio

**Claim**: Let (A, B) be a local optimal partition and let (A\*, B\*) be a global optimal partition.  Then w(A, B) >= Â½ w(A\*, B\*). So Ï is 2. è¯æ˜è¿˜å¥½ï¼Œçœ‹PPT

### Better algo

* [Sahni-Gonzales 1976]  There exists a 2-approximation algorithm for MAX-CUT.
* [Goemans-Williamson 1995]  There exists a 1.1382($\displaystyle =\min_{0\leqslant\theta\leqslant\pi}\frac{\pi(1-\cos\theta)}{2\theta}$)-approximation algorithm for MAX-CUT.
* [HÃ¥stad 1997]  Unless P = NP, no 17/16 approximation algorithm for MAX-CUT.

### Time Opt

To terminitae in polynomial time, stop the algorithm when there are no "big enough" improvements.

**Big-improvement-flip**:  Only choose a node which, when flipped, increases the cut value by at least $\frac{2\epsilon}{|V|}w(A, B)$ï¼Œè¿™é‡ŒÎµæ˜¯ä¸ªå°å€¼

**Claim**:

1. Upon termination, the big-improvement-flip algorithm returns a cut (A, B) so that $(2+\epsilon)w(A, B) \geqslant w(A^*, B^*)$
2. The big-improvement-flip algorithm terminates after at most $O(\frac{|V|}\epsilon \log W)$ flips.

**Proof**(æ„Ÿè§‰ä¸é‡è¦):

1. æ¯æ¬¡flip**è‡³å°‘**å¢åŠ $(1+\frac{\epsilon}{|V|})$å€ï¼Œå…¶å®æ˜¯$(1+\frac{2\epsilon}{|V|})$å€
2. $\frac{|V|}{\epsilon}$æ¬¡flipä¹‹åï¼Œæ€»å¢é•¿è‡³å°‘æ˜¯2å€ã€‚åˆ©ç”¨(1+1/x)^x^ >= 2, å¦‚æœx>=1
3. æ€»é‡ä¸è¶…è¿‡Wï¼Œè€Œcutç¿»å€çš„æ¬¡æ•°ä¸èƒ½è¶…è¿‡logW



ğŸ‘‡ç¡¬å¥—å…¬å¼ï¼Œydså–œæ¬¢æçš„

![](assets/image-20200619115501669.png)